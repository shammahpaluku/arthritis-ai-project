{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "25hUemLXm5RF"
      },
      "id": "25hUemLXm5RF"
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 0] Install dependencies\n",
        "# =========================\n",
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install timm==1.0.9\n",
        "!pip install albumentations==1.4.18 opencv-python-headless==4.10.0.84 pydicom==2.4.4\n",
        "!pip install pandas==2.2.3 scikit-learn==1.5.2 matplotlib==3.9.2 tqdm==4.66.5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UCnNqC1PJsIh",
        "outputId": "9583f4ca-6c93-4e1e-c995-d290b4a655ca"
      },
      "id": "UCnNqC1PJsIh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting timm==1.0.9\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm==1.0.9) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==1.0.9) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm==1.0.9) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm==1.0.9) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm==1.0.9) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.9) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.9) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.9) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.9) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.9) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.9) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm==1.0.9) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm==1.0.9) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm==1.0.9) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm==1.0.9) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm==1.0.9) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm==1.0.9) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm==1.0.9) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm==1.0.9) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==1.0.9) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==1.0.9) (11.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.19\n",
            "    Uninstalling timm-1.0.19:\n",
            "      Successfully uninstalled timm-1.0.19\n",
            "Successfully installed timm-1.0.9\n",
            "Collecting albumentations==1.4.18\n",
            "  Downloading albumentations-1.4.18-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting opencv-python-headless==4.10.0.84\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pydicom==2.4.4\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.18) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.18) (1.16.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.18) (0.25.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.18) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from albumentations==1.4.18) (2.11.7)\n",
            "Collecting albucore==0.0.17 (from albumentations==1.4.18)\n",
            "  Downloading albucore-0.0.17-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting eval-type-backport (from albumentations==1.4.18)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->albumentations==1.4.18) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->albumentations==1.4.18) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->albumentations==1.4.18) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.0->albumentations==1.4.18) (0.4.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.18) (0.4)\n",
            "Downloading albumentations-1.4.18-py3-none-any.whl (224 kB)\n",
            "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.17-py3-none-any.whl (10 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: pydicom, opencv-python-headless, eval-type-backport, albucore, albumentations\n",
            "\u001b[2K  Attempting uninstall: opencv-python-headless\n",
            "\u001b[2K    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "\u001b[2K    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "\u001b[2K      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "\u001b[2K  Attempting uninstall: albucore\n",
            "\u001b[2K    Found existing installation: albucore 0.0.24\n",
            "\u001b[2K    Uninstalling albucore-0.0.24:\n",
            "\u001b[2K      Successfully uninstalled albucore-0.0.24\n",
            "\u001b[2K  Attempting uninstall: albumentations\n",
            "\u001b[2K    Found existing installation: albumentations 2.0.8\n",
            "\u001b[2K    Uninstalling albumentations-2.0.8:\n",
            "\u001b[2K      Successfully uninstalled albumentations-2.0.8\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [albumentations]\n",
            "\u001b[1A\u001b[2KSuccessfully installed albucore-0.0.17 albumentations-1.4.18 eval-type-backport-0.2.2 opencv-python-headless-4.10.0.84 pydicom-2.4.4\n",
            "Collecting pandas==2.2.3\n",
            "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting scikit-learn==1.5.2\n",
            "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting matplotlib==3.9.2\n",
            "  Downloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm==4.66.5\n",
            "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.2) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.2) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.2) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.2) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.2) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.9.2) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m168.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, scikit-learn, pandas, matplotlib\n",
            "\u001b[2K  Attempting uninstall: tqdm\n",
            "\u001b[2K    Found existing installation: tqdm 4.67.1\n",
            "\u001b[2K    Uninstalling tqdm-4.67.1:\n",
            "\u001b[2K      Successfully uninstalled tqdm-4.67.1\n",
            "\u001b[2K  Attempting uninstall: scikit-learn\n",
            "\u001b[2K    Found existing installation: scikit-learn 1.6.1\n",
            "\u001b[2K    Uninstalling scikit-learn-1.6.1:\n",
            "\u001b[2K      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.10.0\n",
            "\u001b[2K    Uninstalling matplotlib-3.10.0:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.9.2 pandas-2.2.3 scikit-learn-1.5.2 tqdm-4.66.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "552b599bfb9a4493b9def4d685cbf8d6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 1] Imports & Config\n",
        "# =========================\n",
        "import os, random, warnings, json\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import pydicom\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "def seed_all(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_all(42)\n",
        "\n",
        "# -------------------------\n",
        "# Device\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# -------------------------\n",
        "# Paths\n",
        "# -------------------------\n",
        "BASE_DIR     = Path(\"/content/drive/MyDrive/osteovision\")  # Change if needed\n",
        "OUT_DIR      = BASE_DIR / \"outputs\"; OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dataset folders\n",
        "TRAIN_DIR    = BASE_DIR / \"train\"\n",
        "VALID_DIR    = BASE_DIR / \"validation\"\n",
        "TEST_DIR     = BASE_DIR / \"test\"\n",
        "\n",
        "# -------------------------\n",
        "# Model / Training parameters\n",
        "# -------------------------\n",
        "NUM_CLASSES   = 2  # Change if your dataset has 3 classes\n",
        "CLASS_NAMES   = [\"normal\", \"osteoarthritis\"]\n",
        "IMG_SIZE      = 512\n",
        "BATCH_SIZE    = 16\n",
        "NUM_WORKERS   = 4\n",
        "\n",
        "MODEL_NAME    = \"tf_efficientnet_b3_ns\"\n",
        "FP16          = True\n",
        "\n",
        "# Learning rates & schedule\n",
        "INIT_LR_HEADS = 1e-3\n",
        "INIT_LR_FULL  = 3e-4\n",
        "EPOCHS_HEADS  = 5\n",
        "EPOCHS_FULL   = 30\n",
        "PATIENCE_LR   = 3\n",
        "PATIENCE_ES   = 7\n",
        "\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "u5xhFVroJtbP"
      },
      "id": "u5xhFVroJtbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 2] Generate DataFrames from folder structure\n",
        "# =========================\n",
        "def folder_to_df(base_dir: Path, class_names: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Scans folders like train/normal, train/osteoarthritis and returns a DataFrame\n",
        "    with columns: file_path, label\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for cls_idx, cls_name in enumerate(class_names):\n",
        "        d = base_dir / cls_name\n",
        "        if not d.exists():\n",
        "            continue\n",
        "        for p in d.rglob(\"*\"):\n",
        "            if p.is_file() and p.suffix.lower() in [\".png\",\".jpg\",\".jpeg\",\".dcm\"]:\n",
        "                rows.append({\"file_path\": str(p), \"label\": cls_idx})\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "# Generate DataFrames\n",
        "df_train = folder_to_df(TRAIN_DIR, CLASS_NAMES)\n",
        "df_valid = folder_to_df(VALID_DIR, CLASS_NAMES)\n",
        "df_test  = folder_to_df(TEST_DIR,  CLASS_NAMES)\n",
        "\n",
        "print(f\"Train images: {len(df_train)} | Validation images: {len(df_valid)} | Test images: {len(df_test)}\")\n"
      ],
      "metadata": {
        "id": "R5QWWITuJz7U"
      },
      "id": "R5QWWITuJz7U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 3] Dataset class, augmentations, and DataLoaders\n",
        "# =========================\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# --- 1️⃣ Read images (PNG/JPG/DICOM) ---\n",
        "def read_image_any(path: str) -> np.ndarray:\n",
        "    path = str(path)\n",
        "    ext = Path(path).suffix.lower()\n",
        "    if ext == \".dcm\":\n",
        "        dcm = pydicom.dcmread(path)\n",
        "        arr = dcm.pixel_array.astype(np.float32)\n",
        "        arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-6)\n",
        "        arr = (arr * 255).clip(0,255).astype(np.uint8)\n",
        "        if arr.ndim == 2:\n",
        "            arr = cv2.cvtColor(arr, cv2.COLOR_GRAY2RGB)\n",
        "        return arr\n",
        "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(path)\n",
        "    if img.ndim == 2:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    else:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "# --- 2️⃣ Augmentations ---\n",
        "train_tfms = A.Compose([\n",
        "    A.LongestMaxSize(IMG_SIZE),\n",
        "    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_REFLECT_101),\n",
        "    A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.85, 1.0), ratio=(0.9, 1.1), p=0.6),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.CoarseDropout(max_holes=1, max_height=IMG_SIZE//10, max_width=IMG_SIZE//10, p=0.25),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "valid_tfms = A.Compose([\n",
        "    A.LongestMaxSize(IMG_SIZE),\n",
        "    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=cv2.BORDER_REFLECT_101),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# --- 3️⃣ Dataset class ---\n",
        "class OAImageDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, transforms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        img = read_image_any(r[\"file_path\"])\n",
        "        if self.transforms:\n",
        "            img = self.transforms(image=img)[\"image\"]\n",
        "        label = int(r[\"label\"])\n",
        "        return img, label\n",
        "\n",
        "# --- 4️⃣ Create Dataset objects & DataLoaders ---\n",
        "train_ds = OAImageDataset(df_train, transforms=train_tfms)\n",
        "valid_ds = OAImageDataset(df_valid, transforms=valid_tfms)\n",
        "test_ds  = OAImageDataset(df_test,  transforms=valid_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "__IK75b0J4xo"
      },
      "id": "__IK75b0J4xo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 4] Model, Loss, Optimizer\n",
        "# =========================\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- 1️⃣ Build model ---\n",
        "def build_model(num_classes: int):\n",
        "    \"\"\"\n",
        "    Creates a timm model with pretrained weights.\n",
        "    \"\"\"\n",
        "    model = timm.create_model(MODEL_NAME, pretrained=True, in_chans=3, num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "model = build_model(NUM_CLASSES).to(device)\n",
        "\n",
        "# --- 2️⃣ Loss function ---\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- 3️⃣ Optimizer ---\n",
        "def make_optimizer(model, lr):\n",
        "    \"\"\"\n",
        "    AdamW optimizer only for parameters that require grad.\n",
        "    \"\"\"\n",
        "    return optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "                       lr=lr, weight_decay=1e-4)\n",
        "\n",
        "# --- Optional: print model summary (requires torchinfo) ---\n",
        "# from torchinfo import summary\n",
        "# summary(model, input_size=(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE))\n"
      ],
      "metadata": {
        "id": "XCMea50sKBov"
      },
      "id": "XCMea50sKBov",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 5] Utilities: metrics, early stopping, checkpointing\n",
        "# =========================\n",
        "import torch\n",
        "\n",
        "# --- 1️⃣ Accuracy metric ---\n",
        "def accuracy(logits, y):\n",
        "    \"\"\"\n",
        "    Returns simple accuracy between logits and true labels.\n",
        "    \"\"\"\n",
        "    preds = logits.argmax(1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "# --- 2️⃣ EarlyStopping ---\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=PATIENCE_ES, mode=\"min\", delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.delta = delta\n",
        "        self.best = None\n",
        "        self.num_bad = 0\n",
        "        self.stop = False\n",
        "\n",
        "    def __call__(self, value):\n",
        "        if self.best is None:\n",
        "            self.best = value\n",
        "            return\n",
        "        improve = (value < self.best - self.delta) if self.mode == \"min\" else (value > self.best + self.delta)\n",
        "        if improve:\n",
        "            self.best = value\n",
        "            self.num_bad = 0\n",
        "        else:\n",
        "            self.num_bad += 1\n",
        "            if self.num_bad >= self.patience:\n",
        "                self.stop = True\n",
        "\n",
        "# --- 3️⃣ Checkpoint saving/loading ---\n",
        "def save_ckpt(path, model, optimizer, epoch, best_metric):\n",
        "    torch.save({\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "        \"best_metric\": best_metric\n",
        "    }, path)\n",
        "\n",
        "def load_ckpt(path, model, optimizer=None):\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    if optimizer is not None and \"optimizer\" in ckpt:\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    return ckpt.get(\"epoch\", 0), ckpt.get(\"best_metric\", None)\n"
      ],
      "metadata": {
        "id": "LtdTbsSmKG_S"
      },
      "id": "LtdTbsSmKG_S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 6] Train & Eval Epoch Loops (AMP + FP16)\n",
        "# =========================\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def run_epoch(model, loader, optimizer=None, scaler=None):\n",
        "    \"\"\"\n",
        "    Runs one epoch of training or evaluation.\n",
        "    - If optimizer is provided → training\n",
        "    - If optimizer is None → evaluation only\n",
        "    \"\"\"\n",
        "    is_train = optimizer is not None\n",
        "    model.train(is_train)\n",
        "\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "    all_logits, all_y = [], []\n",
        "\n",
        "    for imgs, labels in tqdm(loader, disable=False):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with autocast(enabled=FP16):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            if FP16:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        total_acc  += accuracy(logits, labels) * imgs.size(0)\n",
        "        all_logits.append(logits.detach().cpu())\n",
        "        all_y.append(labels.detach().cpu())\n",
        "        n += imgs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / max(n,1)\n",
        "    avg_acc  = total_acc  / max(n,1)\n",
        "    all_logits = torch.cat(all_logits)\n",
        "    all_y      = torch.cat(all_y)\n",
        "\n",
        "    auc = None\n",
        "    try:\n",
        "        proba = all_logits.softmax(1).numpy()\n",
        "        auc = roc_auc_score(all_y.numpy(), proba, multi_class=\"ovr\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return {\"loss\": avg_loss, \"acc\": avg_acc, \"auc\": auc}\n",
        "\n",
        "# --- Example usage ---\n",
        "# scaler = GradScaler(enabled=FP16)\n",
        "# optimizer = make_optimizer(model, INIT_LR_HEADS)\n",
        "# train_metrics = run_epoch(model, train_loader, optimizer, scaler)\n",
        "# val_metrics   = run_epoch(model, valid_loader)\n"
      ],
      "metadata": {
        "id": "VhT_ePbVLuS1"
      },
      "id": "VhT_ePbVLuS1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 7] Stage 1: Train Heads Only\n",
        "# =========================\n",
        "# Freeze all except head/fc/classifier layers\n",
        "for n, p in model.named_parameters():\n",
        "    p.requires_grad = (\"fc\" in n or \"classifier\" in n or \"head\" in n)\n",
        "\n",
        "optimizer = make_optimizer(model, INIT_LR_HEADS)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=PATIENCE_LR, factor=0.5, verbose=True)\n",
        "early = EarlyStopping(patience=PATIENCE_ES, mode=\"min\")\n",
        "scaler = GradScaler(enabled=FP16)\n",
        "\n",
        "best_val = 1e9\n",
        "CKPT_HEADS = OUT_DIR / \"model_heads_best.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS_HEADS+1):\n",
        "    tr = run_epoch(model, train_loader, optimizer, scaler)\n",
        "    va = run_epoch(model, valid_loader)\n",
        "    scheduler.step(va[\"loss\"])\n",
        "    early(va[\"loss\"])\n",
        "    if va[\"loss\"] < best_val:\n",
        "        best_val = va[\"loss\"]\n",
        "        save_ckpt(CKPT_HEADS, model, optimizer, epoch, best_val)\n",
        "    print(f\"[Heads][{epoch}] train {tr['loss']:.4f}/{tr['acc']:.4f} | valid {va['loss']:.4f}/{va['acc']:.4f} auc {va['auc']}\")\n",
        "    if early.stop:\n",
        "        print(\"Early stopping (heads)\")\n",
        "        break\n",
        "\n",
        "# Load best heads checkpoint\n",
        "_ = load_ckpt(CKPT_HEADS, model)\n"
      ],
      "metadata": {
        "id": "OXIV4-MUL1EY"
      },
      "id": "OXIV4-MUL1EY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 8] Stage 2: Full Fine-Tune\n",
        "# =========================\n",
        "# Unfreeze all layers\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer = make_optimizer(model, INIT_LR_FULL)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=PATIENCE_LR, factor=0.5, verbose=True)\n",
        "early = EarlyStopping(patience=PATIENCE_ES, mode=\"min\")\n",
        "scaler = GradScaler(enabled=FP16)\n",
        "\n",
        "best_val = 1e9\n",
        "CKPT_FULL = OUT_DIR / \"model_full_best.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS_FULL+1):\n",
        "    tr = run_epoch(model, train_loader, optimizer, scaler)\n",
        "    va = run_epoch(model, valid_loader)\n",
        "    scheduler.step(va[\"loss\"])\n",
        "    early(va[\"loss\"])\n",
        "    if va[\"loss\"] < best_val:\n",
        "        best_val = va[\"loss\"]\n",
        "        save_ckpt(CKPT_FULL, model, optimizer, epoch, best_val)\n",
        "    print(f\"[Full][{epoch}] train {tr['loss']:.4f}/{tr['acc']:.4f} | valid {va['loss']:.4f}/{va['acc']:.4f} auc {va['auc']}\")\n",
        "    if early.stop:\n",
        "        print(\"Early stopping (full)\")\n",
        "        break\n",
        "\n",
        "# Load best full checkpoint\n",
        "_ = load_ckpt(CKPT_FULL, model)\n"
      ],
      "metadata": {
        "id": "eiK-DRRKL-xe"
      },
      "id": "eiK-DRRKL-xe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 9] Optional Cosine Cooldown\n",
        "# =========================\n",
        "EPOCHS_COOLDOWN = 5\n",
        "optimizer = make_optimizer(model, INIT_LR_FULL * 0.3)\n",
        "cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_COOLDOWN)\n",
        "scaler = GradScaler(enabled=FP16)\n",
        "\n",
        "for epoch in range(1, EPOCHS_COOLDOWN+1):\n",
        "    tr = run_epoch(model, train_loader, optimizer, scaler)\n",
        "    va = run_epoch(model, valid_loader)\n",
        "    cosine.step()\n",
        "    print(f\"[Cooldown][{epoch}] train {tr['loss']:.4f}/{tr['acc']:.4f} | valid {va['loss']:.4f}/{va['acc']:.4f} auc {va['auc']}\")\n"
      ],
      "metadata": {
        "id": "Sm8DJ-CgMFfW"
      },
      "id": "Sm8DJ-CgMFfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 10] Evaluation on Validation Set\n",
        "# =========================\n",
        "model.eval()\n",
        "ys, ps = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS):\n",
        "        imgs = imgs.to(device)\n",
        "        logits = model(imgs).softmax(1).cpu().numpy()\n",
        "        ps.append(logits)\n",
        "        ys.append(labels.numpy())\n",
        "\n",
        "ps = np.concatenate(ps)\n",
        "ys = np.concatenate(ys)\n",
        "preds = ps.argmax(1)\n",
        "\n",
        "# Classification report\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(ys, preds, target_names=CLASS_NAMES))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(ys, preds))\n",
        "\n",
        "# Macro AUC\n",
        "try:\n",
        "    auc = roc_auc_score(ys, ps, multi_class=\"ovr\")\n",
        "    print(\"Macro AUC:\", auc)\n",
        "except Exception:\n",
        "    print(\"AUC calculation failed (check labels/probabilities).\")\n"
      ],
      "metadata": {
        "id": "qg6iu_GmMRys"
      },
      "id": "qg6iu_GmMRys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 11] Export Weights + Metadata\n",
        "# =========================\n",
        "EXPORT_DIR = OUT_DIR / \"export\"\n",
        "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save model state dict\n",
        "torch.save(model.state_dict(), EXPORT_DIR / \"model_state_dict.pt\")\n",
        "\n",
        "# Save metadata for inference\n",
        "meta = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"img_size\": IMG_SIZE,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"class_names\": CLASS_NAMES,\n",
        "    \"normalization\": \"albumentations.Normalize() default (ImageNet)\",\n",
        "}\n",
        "with open(EXPORT_DIR / \"meta.json\", \"w\") as f:\n",
        "    json.dump(meta, f, indent=4)\n",
        "\n",
        "print(\"Exported model and metadata to:\", EXPORT_DIR)\n"
      ],
      "metadata": {
        "id": "GSociuEPMUgo"
      },
      "id": "GSociuEPMUgo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# [SEGMENT 12] Simple Inference Helper\n",
        "# =========================\n",
        "def infer_one(path: str):\n",
        "    \"\"\"\n",
        "    Predicts class probabilities for a single image.\n",
        "    Returns a dict {class_name: probability}.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    img = read_image_any(path)\n",
        "    tfm = valid_tfms(image=img)[\"image\"].unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(tfm).softmax(1).cpu().numpy()[0]\n",
        "    return {CLASS_NAMES[i]: float(logits[i]) for i in range(NUM_CLASSES)}\n",
        "\n",
        "# --- Example usage ---\n",
        "# sample_path = df_valid.iloc[0][\"file_path\"]\n",
        "# print(infer_one(sample_path))\n"
      ],
      "metadata": {
        "id": "IkFLXN0DMcV7"
      },
      "id": "IkFLXN0DMcV7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}